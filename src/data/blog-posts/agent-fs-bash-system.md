---
title: "Building a Filesystem + Bash Based Agentic Memory System (Part 1)"
slug: agentic-memory-filesystem-part-1
publishDate: 16 Jan 2026
description: "Part 1 of a 3-part series exploring how to give agents filesystem and bash access. Research, patterns, and design goals for building a sandboxed execution environment."
---

# Building a Filesystem + Bash Based Agentic Memory System (Part 1)

*Part 1 of 3: Research, Patterns, and Design Goals*

---

A few days ago, I wrote about [how I reduced my agent's token consumption by 83%](https://dev.to/jeremiahbarias/how-i-reduced-my-agents-token-consumption-by-83-57nh) by implementing a `ToolFilterManager` that dynamically selects which tools to expose based on query relevance. That tackled the first major pattern from Anthropic's [Advanced Tool Use](https://www.anthropic.com/engineering/advanced-tool-use) articleâ€”the **tool search tool**.

But that article describes *three* patterns, and I've been eyeing the second one: **programmatic tool calling**.

The idea is to let Claude "orchestrate tools through code rather than through individual API round-trips." Instead of the model making 20 sequential tool calls (each requiring an inference pass), it writes a single code block that executes all of them, processing outputs in a sandboxed environment without inflating context. Anthropic reports a 37% token reduction on complex tasks with this approach.

This got me thinking: what if we took this further? What if instead of code execution, we gave agents direct filesystem and bash access?

Welcome to Part 1 of this rabbit hole.

## What are we talking about?

- [Why Filesystem + Bash?](#why-filesystem--bash)
- [Existing Work](#existing-work)
- [How It Works: Traditional vs Filesystem-Based](#how-it-works-traditional-vs-filesystem-based)
- [Bridging the Gap: MCP as CLI](#bridging-the-gap-mcp-as-cli)
- [Design Goals for My Experiment](#design-goals-for-my-experiment)
- [What This Isn't](#what-this-isnt)
- [Next Up](#next-up)

---

## Why Filesystem + Bash?

Vercel published a piece on [building agents with filesystems and bash](https://vercel.com/blog/how-to-build-agents-with-filesystems-and-bash) that crystallized something I'd been mulling over. Their core insight:

> LLMs have been trained on massive amounts of code.

Models already know how to `grep`, `cat`, `find`, and `ls`. They've seen millions of examples of bash usage during training. You don't need to teach them your custom `SearchCodebase` toolâ€”they already know `grep -r "pricing objection" ./transcripts/`.

Their results were compelling: a sales call summarization agent went from $1.00 to $0.25 per call on Claude Opus while *improving* output quality. That's not a typoâ€”cheaper AND better.

The reason? **Contextual precision.** Vector search gives you semantic approximations. Prompt stuffing hits token limits. But `grep -r` returns exactly what you asked for, nothing more.

If you've used Claude Code, you've seen this pattern in action. The agent doesn't call abstract toolsâ€”it has a filesystem and runs commands against it. The model thinks in `cat`, `head`, `tail`, and `jq`, not `ReadFile(path="/foo/bar")`.

## Existing Work

I'm not the first person down this path.

**[AgentFS](https://docs.turso.tech/agentfs/introduction)** from Turso is a filesystem abstraction built on SQLite. Their pitch: "copy-on-write isolation, letting agents safely modify files while keeping your original data untouched." Everything lives in a single portable SQLite databaseâ€”easy to snapshot, share, and audit. They've built CLI wrappers and SDKs for TypeScript, Python, and Rust. It's marked as ALPHA and explicitly not for production, but the architecture is interesting.

**Claude Code** is the obvious reference implementation. Anthropic gave their coding agent real filesystem access with sandboxing, and it works remarkably well. The agent naturally uses bash patterns it learned during training.

**Vercel's `bash-tool`** provides sandboxed bash execution alongside their AI SDK. Their examples show domain-to-filesystem mappings: customer support data organized by customer ID with tickets and conversations as nested files, sales transcripts alongside CRM records.

**[mcp-cli](https://www.philschmid.de/mcp-cli)** and **[mcptools](https://github.com/f/mcptools)** enable calling MCP servers from the command line. This is the missing linkâ€”it lets agents invoke MCP tools via bash and redirect output to files, bridging the gap between structured tool definitions and filesystem-based execution.

## How It Works: Traditional vs Filesystem-Based

Before diving deeper, let me illustrate the fundamental difference between these approaches.

### Traditional Agentic Tool Calling

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  TRADITIONAL TOOL CALLING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  User Query â”€â”€â”€â”€â”€â”€â–¶ Agent (sends ALL 16 tool definitions)
                                      â”‚
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚      LLM      â”‚
                              â”‚ "I'll use     â”‚
                              â”‚ search_docs & â”‚
                              â”‚ query_databaseâ”‚
                              â”‚ tools"        â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                         Agent Executes Tools
                         search_docs("pricing")
                         query_database("customers")
                                      â”‚
                                      â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  RAW OUTPUT (1000s of tokens!)â”‚
                      â”‚  [full doc contents,          â”‚
                      â”‚   all 500 DB rows...]         â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚      LLM      â”‚
                              â”‚  (processes   â”‚
                              â”‚   ENTIRE      â”‚
                              â”‚   output)     â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Response    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Problems:
  â”œâ”€â”€ ðŸ”´ All tool definitions sent every request (5,888 tokens just for schemas!)
  â”œâ”€â”€ ðŸ”´ Full tool output dumped into context (DB query = 500 rows in context)
  â””â”€â”€ ðŸ”´ Each tool call = 1 inference round-trip
```

### Filesystem + Bash Based Tool Calling

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  FILESYSTEM + BASH TOOL CALLING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  User Query â”€â”€â”€â”€â”€â”€â–¶ Agent (sends sandbox tool + fs structure)
                                      â”‚
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚      LLM      â”‚
                              â”‚ "I'll explore â”‚
                              â”‚  the data:    â”‚
                              â”‚  ls, cat..."  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                                   â”‚
            â–¼                                                   â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
  â”‚   Sandbox Execution   â”‚                                     â”‚
  â”‚   $ ls ./customers/   â”‚                                     â”‚
  â”‚   > acme/ globex/     â”‚                                     â”‚
  â”‚     initech/ ...      â”‚â”€â”€â”€â”€â”€â”€â”                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                              â”‚
                                 â”‚  (output written to file     â”‚
                                 â”‚   or returned as path)       â”‚
                                 â–¼                              â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
                  â”‚           LLM              â”‚                â”‚
                  â”‚  "Found customers. Now:    â”‚                â”‚
                  â”‚   grep -r 'pricing' ./docs â”‚                â”‚
                  â”‚   | head -20"              â”‚                â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                â”‚                               â”‚
                                â–¼                               â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
                  â”‚   Sandbox Execution   â”‚                     â”‚
                  â”‚   $ grep -r 'pricing' â”‚                     â”‚
                  â”‚     ./docs | head -20 â”‚                     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
                                â”‚                               â”‚
                                â–¼                               â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
                  â”‚           LLM              â”‚                â”‚
                  â”‚  "Need more detail on      â”‚                â”‚
                  â”‚   enterprise tier:         â”‚                â”‚
                  â”‚   awk '/enterprise/,/---/' â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚     ./docs/pricing.md"     â”‚     (loop until
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      sufficient
                                â”‚                     context)
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Response   â”‚
                        â”‚  (with only  â”‚
                        â”‚   relevant   â”‚
                        â”‚   context)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Benefits:
  â”œâ”€â”€ ðŸŸ¢ Minimal tool definitions (just "sandbox" tool)
  â”œâ”€â”€ ðŸŸ¢ Agent controls what enters context (grep, head, awk filter results)
  â”œâ”€â”€ ðŸŸ¢ LLM already knows bash (trained on millions of examples)
  â””â”€â”€ ðŸŸ¢ Composable commands (pipes, redirects, filters)
```

### The Key Insight

The traditional approach treats the LLM as a passive consumerâ€”it requests data and gets *everything* back. The filesystem approach treats the LLM as an active explorerâ€”it navigates, filters, and retrieves only what it needs.

```
Traditional:    "Give me all the data, I'll figure it out"
                 â””â”€â”€ Context explodes, tokens burn ðŸ”¥

Filesystem:     "Let me look around and grab what I need"
                 â””â”€â”€ Context stays lean, costs drop ðŸ“‰
```

## Bridging the Gap: MCP as CLI

The diagrams above assume files already exist in the sandbox. But where do they come from?

This is where MCP CLI tools bridge the gap. Instead of MCP servers returning results directly into the LLM's context, they can be invoked as bash commands that write output to files.

### MCP as CLI Commands

Several tools enable calling MCP servers from the command line:

**[mcp-cli](https://www.philschmid.de/mcp-cli)** by Phil Schmid uses a clean syntax:
```bash
# List available servers and tools
mcp-cli

# Inspect a tool's schema
mcp-cli filesystem/read_file

# Execute a tool
mcp-cli filesystem/read_file '{"path": "./README.md"}'
```

**[mcptools](https://github.com/f/mcptools)** offers similar functionality:
```bash
mcp call read_file --params '{"path":"README.md"}' npx -y @modelcontextprotocol/server-filesystem ~
```

### The Integration Pattern

Here's how traditional tools integrate with the filesystem approach:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  DATA INGESTION: MCP â†’ SANDBOX FILESYSTEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”Œâ”€ LLM decides it needs customer data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚
  â”‚  "I need to query the database for enterprise customers.
  â”‚   Let me fetch that data into my workspace."
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                               â”‚
                               â–¼
  â”Œâ”€ SANDBOX EXECUTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚
  â”‚  $ mcp-cli database/query_customers '{"tier": "enterprise"}' \
  â”‚      > ./sandbox/data/customers.json
  â”‚
  â”‚  $ mcp-cli vectorstore/search '{"query": "pricing policy"}' \
  â”‚      > ./sandbox/docs/pricing_results.json
  â”‚
  â”‚  $ mcp-cli brave-search/web_search '{"query": "competitor pricing"}' \
  â”‚      > ./sandbox/research/competitors.json
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                               â”‚
                               â”‚  (data now exists as files)
                               â–¼
  â”Œâ”€ SANDBOX FILESYSTEM STATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚
  â”‚  ./sandbox/
  â”‚  â”œâ”€â”€ data/
  â”‚  â”‚   â””â”€â”€ customers.json          # 500 customer records
  â”‚  â”œâ”€â”€ docs/
  â”‚  â”‚   â””â”€â”€ pricing_results.json    # vectorstore search results
  â”‚  â””â”€â”€ research/
  â”‚      â””â”€â”€ competitors.json        # web search results
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                               â”‚
                               â–¼
  â”Œâ”€ LLM explores with bash (only pulls what it needs into context) â”€â”€â”€
  â”‚
  â”‚  $ jq '.customers | length' ./sandbox/data/customers.json
  â”‚  > 500
  â”‚
  â”‚  $ jq '.customers[] | select(.revenue > 1000000) | .name' \
  â”‚      ./sandbox/data/customers.json | head -10
  â”‚  > "Acme Corp"
  â”‚  > "Globex Inc"
  â”‚  > ...
  â”‚
  â”‚  $ grep -l "enterprise" ./sandbox/docs/*.json
  â”‚  > ./sandbox/docs/pricing_results.json
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Why This Matters

The traditional approach would send all 500 customer records directly into context. With filesystem-based execution:

1. **MCP call writes to file** â†’ Data exists but isn't in context yet
2. **Agent uses `jq` to count** â†’ Only "500" enters context (3 tokens)
3. **Agent filters with `jq`** â†’ Only 10 company names enter context (~30 tokens)
4. **Agent got what it needed** â†’ Instead of 500 records (~50,000 tokens)

Phil Schmid's [research on mcp-cli](https://www.philschmid.de/mcp-cli) showed this pattern reduces tool-related token consumption from ~47,000 tokens to ~400 tokensâ€”**a 99% reduction**â€”because agents discover and use tools just-in-time rather than loading all definitions upfront.

### The Complete Flow

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  COMPLETE FILESYSTEM + MCP FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  User Query: "Which enterprise customers mentioned pricing concerns?"
                               â”‚
                               â–¼
  â”Œâ”€ STEP 1: Fetch data via MCP CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚
  â”‚ $ mcp-cli database/query_customers '{"tier":"enterprise"}' \
  â”‚     > ./data/customers.json
  â”‚
  â”‚ $ mcp-cli crm/get_conversations '{"customer_ids":"$CUSTOMER_IDS"}' \
  â”‚     > ./data/conversations.json
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                               â”‚
                               â–¼
  â”Œâ”€ STEP 2: Explore with bash â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚
  â”‚ $ jq -r '.[] | .id' ./data/customers.json | wc -l
  â”‚ > 47
  â”‚
  â”‚ $ grep -l "pricing" ./data/conversations.json
  â”‚ > (matches found)
  â”‚
  â”‚ $ jq '.[] | select(.text | contains("pricing")) | {customer, text}' \
  â”‚     ./data/conversations.json > ./analysis/pricing_mentions.json
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                               â”‚
                               â–¼
  â”Œâ”€ STEP 3: Extract only relevant context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚
  â”‚ $ cat ./analysis/pricing_mentions.json | head -50
  â”‚ > [{"customer": "Acme", "text": "pricing seems high..."},
  â”‚ >  {"customer": "Globex", "text": "need better pricing..."}]
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                               â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Response   â”‚
                        â”‚  (informed   â”‚
                        â”‚   by ~50     â”‚
                        â”‚   relevant   â”‚
                        â”‚   lines)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Token savings:
  â”œâ”€â”€ Without filesystem: 47 customers Ã— 20 conversations Ã— ~500 tokens = 470,000 tokens
  â””â”€â”€ With filesystem: ~200 tokens (just the relevant pricing mentions)
```

## Design Goals for My Experiment

I want to build something that integrates with [Holodeck](https://github.com/justinbarias/holodeck-ai), which uses Semantic Kernel for agent orchestration. Here's what I'm aiming for:

### 1. Filesystem Security

Letting LLMs run bash commands on your actual filesystem is... not great. The horror stories write themselves.

My approach:
- **Copy-on-write isolation.** Like AgentFS, the agent operates in a sandboxed directory. Writes don't touch original files until explicitly committed.
- **Audit logging.** Every file operation gets logged. Every. Single. One. AgentFS makes this queryable, and I want the sameâ€”know what the agent did, when, and be able to roll it back.
- **Path restrictions.** The agent only sees paths within its sandbox. No `rm -rf /` accidents, no reading `~/.ssh/`.

This is non-negotiable for anything beyond toy experiments.

### 2. Token and Context Reduction

This is where the programmatic tool calling pattern really shines.

In traditional tool calling:
1. Model requests tool call
2. Tool executes
3. **Entire output goes back into context**
4. Model processes output
5. Repeat

Query a database with 1000 rows? That's 1000 rows in your context window. Every. Single. Time.

The filesystem pattern flips this:
- Command outputs get written to files
- To access results, the agent runs CLI commands: `head -20 results.json`, `jq '.users[] | .name' data.json`, `grep -c "error" logs.txt`
- The agent pulls in only what it needs, when it needs it, in the format it needs it

This is how Claude Code handles large codebases without blowing through context limits. It's also why Vercel saw their costs drop 75%.

### 3. Integration with Semantic Kernel Tool Calling

Here's where I want to experiment. Holodeck already has tool definitionsâ€”vectorstore searches, MCP servers, custom functions. What if these could execute in "filesystem mode"?

Imagine a `search_knowledge_base` tool that, instead of returning results directly:
1. Runs as a subprocess
2. Writes results to `./sandbox/outputs/search_001.json`
3. Returns just the path to the agent
4. Lets the agent `cat` or `jq` the file as needed

You get structured tool definitions for discoverability (the model knows what tools exist), but filesystem semantics for execution (the model controls what data actually enters context).

This could layer nicely with the tool search pattern I already built. Filter tools dynamically, *then* execute them in a sandboxed filesystem. Best of both worlds.

What might this look like in practice? Today, Holodeck tools are defined like this:

```yaml
tools:
  - name: knowledge_search
    type: vectorstore
    config:
      index: product-docs

  - name: brave_search
    type: mcp
    server: brave-search
```

What if we added an execution mode?

```yaml
tools:
  - name: knowledge_search
    type: vectorstore
    config:
      index: product-docs
    execution:
      mode: filesystem              # NEW: execute via CLI, write to file
      output_dir: ./sandbox/search

  - name: brave_search
    type: mcp
    server: brave-search
    execution:
      mode: filesystem
      output_dir: ./sandbox/web
```

The agent would then call these as CLI commands:
```bash
$ holodeck-tool knowledge_search '{"query": "pricing"}' > ./sandbox/search/001.json
$ holodeck-tool brave_search '{"query": "competitor analysis"}' > ./sandbox/web/001.json
```

Same tool definitions for discoverability. Filesystem semantics for execution. The agent still knows what tools exist (via the tool search pattern from my previous post), but now it controls *when* and *how much* of the output enters context.

### 4. Multi-Platform Support

I'm on macOS. Most servers run Linux. Some people poor souls use Windows.

The goal is cross-platform support, which means:
- No macOS-specific sandboxing (sorry, `sandbox-exec`)
- Abstracting filesystem operations through a clean interface
- Probably leaning on Docker for production isolation

This is the stretch goal. I'll be happy if macOS and Linux work cleanly.

## What This Isn't

To be clear: this is an experiment. I'm not replacing Holodeck's core execution model with bash. The standard tool calling flow works great for most use cases, and the tool search pattern I built already handles the "too many tools" problem.

What I'm building is an *additional* capabilityâ€”a `sandbox` tool that agents can use when they need filesystem-style access for memory-intensive or retrieval-heavy tasks. Think of it as giving your agent a scratchpad with Unix superpowers.

The eventual API might look something like:

```yaml
tools:
  - name: sandbox
    type: sandbox
    config:
      base_path: ./workspace
      allowed_commands: [cat, grep, ls, head, tail, find, jq, awk]
      audit_log: ./logs/sandbox.log
      copy_on_write: true
```

But that's getting ahead of myself. Implementation is for Part 2.

## Next Up

In **Part 2**, I'll dig into implementation details:
- Setting up the sandboxed filesystem
- Copy-on-write semantics (probably borrowing ideas from AgentFS)
- The command execution layer with proper escaping and timeouts
- Audit logging and rollback

**Part 3** will cover Semantic Kernel integrationâ€”making existing tools execute in "filesystem mode" and exposing the whole thing as a Holodeck tool.

If you've built something similar or have thoughts on the approach, I'd love to hear about it.

---

*This post is part of a series on building filesystem-based agentic memory systems. Read my previous post on [reducing token consumption with tool search](https://dev.to/jeremiahbarias/how-i-reduced-my-agents-token-consumption-by-83-57nh) for context on the first pattern I implemented.*
